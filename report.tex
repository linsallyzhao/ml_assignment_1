\documentclass{article}
\usepackage[left=2cm,right=2cm,top=1cm,bottom=1cm,a3paper]{geometry}
\usepackage{pgfplots}
\usepackage{parskip}
\usepackage{caption}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[]{mathtools}
\usepackage{float}
\restylefloat{table}

\setlength{\parindent}{0mm}

\pgfplotsset{compat=1.15}

\newcommand{\cia}[1]{\resizebox{\textwidth}{!}{\input{#1}}}
\newcommand{\ciapdf}[1]{\vspace*{-\parskip}\begin{center}\resizebox{0.75\textwidth}{!}{\includegraphics[width=0.75\textwidth]{#1}}\end{center}}
\newcommand{\halfhalf}[2]{\begin{multicols}{2}\includegraphics[width=0.5\textwidth]{#1} \includegraphics[width=0.5\textwidth]{#2}\end{multicols}}

\begin{document}

\title{Assignment one: supervised learning}
\author{Lin Zhao (16010906)}
\date{\today}
\maketitle

\newpage

\section*{Task 1}

\subsection*{Introduction}

The dataset used for this task is from Schularick and Taylor (2012,
``Credit Booms Gone Bust''). It is an annual dataset covering 14
countries and 140 years. Among the variables they collected, the most
important one is the yearly aggregate bank loans, which is
the main soure of predictive power. The variable of interest is
CrisisST which takes a value of 1 when there is a financial crisis and 0
otherwise.

Following the guidance of Schularick and Taylor (2012), I explored the
relationships between several macro variables within two
eras of finance capitalism, tested the predictive power of different
macro variables, and compared predictive power of different supervised
learning methods. The last part of above is also inspired by the
Fricke (2017, ``Financial Crisis Prediction: A Model Comparison'').

The methods used are logistic regression, classification
tree, classification forest, and SVM. The major criteria
used here is the area under receiver operating curve, and the secondary criteria
is the confusion matrix. The major validation method is the modified cross
validation method  as mentioned in the Fricke paper that takes time order into
consideration.

\subsection*{Data description and cleaning}

To explore the changing features between the two eras, I firstly created
variables of interest: credit to GDP ratio, bank asset to GDP ratio,
money to GDP ratio, credit to money ratio, and bank asset to money
ratio. To see the distinctive trends in different historical periods, I
regrouped the whole dataset by years and took the mean value of each variable
each year. Ploting mean values of the ratios above against time, I
recovered Figure 1 and 2 in Schularick and Taylor (2012).

Figure 1 shows that bank loans, bank assets and broad money supply remain
steady related to the size of economy represented by GDP
before the WW2 period. After the war, the money to GDP ratio stays flat
while the other two start to increase dramatically. In figure 2, we
can see that the loan to money ratio and bank asset to money ratio start
to take off after the disturbance of WW2, implying that credit starts to
grow faster than broad money supply, and that no steady relationship between
the two can be found in this period.

\ciapdf{Figure_2.pdf}

\begin{quote}
Figure 1: Aggregates relative to GDP (year effects)
\end{quote}

\ciapdf{Figure_1.pdf}

\begin{quote}
Figure 2: Aggregates relative to broad money (year effects)
\end{quote}

To study financial crises caused by the internals of the economic system,
we need to
exclude the crises caused by the two world wars. As performed in the Schularick and
Taylor paper, I excluded the war periods (1914 to 1919 and 1939 to 1947)
and the German crisis after WW1 (1920 to 1925). I then divided the cleaned up
dataset into pre- and post-war periods, replicating the upper panel of
table 1 in the paper.

\subsubsection*{\centering{}Annual summary statistics pre- and post-war}

\begin{table}[H]
    \begin{center}\begin{tabular}{|l|l|l|l|l|l|}
    \hline
          & credit\_to\_GDP & bankAsset\_to\_GDP & money\_to\_GDP & credit\_to\_money & bank\_asset\_to\_money \\ \hline
          & Pre-war         &                    &                &                   &                        \\ \hline
    count & 685             & 611                & 736            & 662               & 580                    \\ \hline
    mean  & 0.408977        & 0.714051           & 0.533292       & 0.735337          & 1.282481               \\ \hline
    std   & 0.359888        & 0.447337           & 0.207534       & 0.449343          & 0.566104               \\ \hline
          & Post-war        &                    &                &                   &                        \\ \hline
    count & 831             & 828                & 834            & 833               & 831                    \\ \hline
    mean  & 0.546975        & 1.013497           & 0.645801       & 0.838012          & 1.575839               \\ \hline
    std   & 0.423878        & 0.668770           & 0.240497       & 0.494226          & 0.752540               \\ \hline
    \end{tabular}\end{center}
\end{table}

More than half of the countries in the dataset are from
Europe, so I also excluded all the observations from the post WW1 period (1920 to
1925) since all of the crises which happened within
that period could have been caused by WW1 rather than the economic system itself. I
only kept the variables that potentially have strong predictive powers
according to the results and the robustness test from the paper.

After dropping any row with missing values, we are left with 1433 observations,
and 59 out of these are crisis
events.

\subsection*{Supervised learning methods for classification}

\subsubsection*{Logistic regression and choice of explanatory variables}

As defined in the Schularick and Taylor paper, I created the CPI nomalized bank
loan and took the difference of log values of this variable as the credit environment changed. This variable will be called credit change. I also
took the credit to GDP ratio as one potential
explanatory variable based on the robustness test presented in the paper. This variable
lagged 1 year will be called credit size. After
assigning each country its lagged 1 to 5 credit change and credit size for each year, I
sorted the dataset by time to make sure that when fitting a model, it is
not trying to predict the crises of the 1960s with 1990s data.

I started the analysis by using lag 1 to lag 5 credit change to fit a
logistic regression model. The AUC is slightly higher than 0.5 for the whole
dataset and for the pre-war dataset, and significantly higher than 0.5
for the post-war dataset. Since in the paper, lag 2 credit change is the only
lagged variable that
is significant, I also fitted logistic regression with only lag 2 data
and the model fit slightly better for both the whole set and the pre-war period
in respect of AUC. The change in the post-war period is ambiguous and
credit size seems to add predictive power to the post-war period. The lag 2 credit
change is indeed the
main source of information.

\subsubsection*{\centering{}In-sample AUC for logistic regression with lag 1 to 5 credit change}

\begin{table}[H]
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.5861360718870345 & 0.5762987012987013 & 0.7608543417366948 \\ \hline
    with credit size    & 0.5892169448010269 & 0.5892169448010269 & 0.615546218487395 \\ \hline
    \end{tabular}\end{center}
    \caption{in-sample logistic regression trained with whole time period,
    pre-war period and post-war period with lag 1 to 5 credit change as major
    explanatory variable.}
\end{table}

\subsubsection*{\centering{}In-sample AUC for logistic regression with lag 2 credit change}

\begin{table}[H]
    \caption{in-sample logistic regression trained with whole time period,
    pre-war period and post-war period with lag 2 credit change as major
    explanatory variable.}
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.6373277827336704 & 0.6286424526999033 & 0.697533908754624  \\ \hline
    with credit size    & 0.4518906730102092 & 0.5177922018137459 & 0.7200369913686806 \\ \hline
    \end{tabular}\end{center}
\end{table}

I did an out-of-sample test for the choice of variable using
30\% of the dataset as the test set and 70\% as the training set.
The results show that models fitted with lag 2 credit change have
significantly better out-of-sample performance than models fitted with
lag 1 to lag 5 and credit size doesn't seem to add predictive
power to the model. The AUC is reported in the following tables.

\subsubsection*{\centering{}Out-of-sample AUC for logistic regression with lag 1 to lag 5 credit change}

\begin{table}[H]
    \caption{out-of-sample logistic regression trained with 70\% of
    each data set and tested on 30\% of data with lag 1 to lag 5
    credit change as major explanatory variable.}
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.5861360718870345 & 0.5762987012987013 & 0.7608543417366948 \\ \hline
    with credit size    & 0.5892169448010269 & 0.5800865800865801 & 0.615546218487395  \\ \hline
    \end{tabular}\end{center}
\end{table}


\subsubsection*{\centering{}Out-of-sample AUC for logistic regression with lag 2 credit change}

\begin{table}[H]
    \caption{out-of-sample logistic regression trained with 70\% of
    each data set and tested on 30\% of data with lag 2 credit change
    as major explanatory variable.
    }
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.7370988446726572 & 0.7976190476190476 & 0.7461484593837535 \\ \hline
    with credit size    & 0.531193838254172  & 0.6737012987012987 & 0.6355042016806722 \\ \hline
    \end{tabular}\end{center}
\end{table}

\ciapdf{Figure_4.pdf}
\begin{quote}
Figure 3: out-of-sample AUC of logistic fitted with 70\% of
data as training set and 30\% as testing set. All three periods
fitted with lag 2 credit change only.
\end{quote}

Given the above results, only lag 2 credit change and credit
size variables were considered for the rest of this study. This also means that all of the models
compared here will have the
same information as their input and thus this makes the comparison meaningful.

Finally, for logistic regression, I did a modified cross validation as
mentioned in the Fricke paper, dividing the whole dataset into four
equal folds. First, I use fold one to train and test on fold two. Then I
use fold one and two to train and test on fold three, etc. The average
AUC is 0.56129 which is higher than 0.5. The reason to divide the
dataset into 4 folds rather than 5 (as in the Fricke paper) is that
when the fold is too small, due to the sparseness of crisis events,
there might be only one class in the whole training set or test set.

\subsubsection*{Tree and forest}

Next I fitted the data with a classification tree. With maximum depth
equals to 3, here are result of in-sample prediction. It is obvious from
the table that credit size add on predictive power for classification
tree at least for in-sample test.

\subsubsection*{\centering{}AUC for classification tree}

\begin{table}[H]
    \caption{
    in-sample classification tree fitted with lag 2 credit change
    as major explanatory variables
    }
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.6937568143522649 & 0.7030106338903466 & 0.780209617755857  \\ \hline
    with credit size    & 0.7598684210526315 & 0.7878746029553929 & 0.8358199753390876 \\ \hline
    \end{tabular}\end{center}
\end{table}

From the AUC plot we can see that there are much less point on each line
for the tree compared with logistic regression. This is because for
logistic regression, each observation will have its own estimated
probability, but for a tree, the observations belong to the same leaf
will have the same probability.

\ciapdf{Figure_5.pdf}
\begin{quote}
Figure 4: AUC of classification tree fitted with whole time period,
pre-war and post-war period. All three period fitted with lag 2 credit
change and credit size.
\end{quote}

For the choice of maximum depth, we need a good balance between fully using
all the information and avoiding overfitting. To find the maximum depth
that giving highest AUC for each tree, I performed analysis through the
following steps:

\begin{enumerate}
    \item For the whole set, pre-war, and post-war periods, I set up a
          modified cross validation of 5 folds as mentioned in the logistic
          regression part with a maximum depth constraint and record the
          average AUC for each model;
    \item I collected the average AUC of each model for maximum depths from 2 to 50;
    \item I plotted the average AUC against the maximum depth for each model and picked
          the depth that coincided with the highest AUC.
\end{enumerate}


\ciapdf{Figure_6.pdf}
\begin{quote}
Figure 5: Average AUC for the whole, pre- and post-war dataset fitted
with and without credit size plot against maximum depth
\end{quote}

The eventual optimum maximum depths were 3, 3, and 5 respectively.
Based on these values, I fitted the 70\% training set for the whole period and
pre-war datasets with lag 2 credit change, and post-war with lag 2 credit
change and credit size. With 30\% of the total data as a test set, these models show AUC 0.62895,
0.69021 and 0.43697 respectively.

The dramatically different performance between in-sample and
out-of-sample test is expected and indicates overfitting. Given a high enough
max depth, a classification tree can achieve AUC 1.0 in an in-sample test.
However, overfitting will lead to very poor out-of-sample prediction, which
is indicated by the flattening-out in the figure above. Two trees with different maximum depth in the appendex demonstrate
this point.

Using exactly the same method, I performed analysis with classification forest
models. With the same max depth, the in-sample performs better than the tree,
which in general indicates higher predictive power. However, forest is also
vulnerable to overfitting.

\subsubsection*{\centering{}AUC for classification forest}


\begin{table}[H]
    \caption{in-sample classification forest fitted with lag 2 credit change
    as major explanatory variable.}
    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
                        & whole set          & pre-war            & post-war           \\ \hline
    without credit size & 0.7419033105362275 & 0.7823735211526954 & 0.9169852034525278 \\ \hline
    with credit size    & 0.7619994548518189 & 0.8678359342632234 & 0.8669852034525277 \\ \hline
    \end{tabular}\end{center}
\end{table}

\ciapdf{Figure_7.pdf}
\begin{quote}
Figure 6: AUC of classification forest fitted with whole time period,
pre-war and post-war period. The two periods are fitted with lag 2 credit
change and credit size, while the post-war is fitted with lag 2 credit change
only.
\end{quote}

With optimal max depth picked up by the same method, as in the tree
analysis, datasets are fitted with lag 2 credit change and/or credit
size. Testing with 30\% data in the dataset, the AUCs are 0.62426,
0.73593, and 0.71709 for whole, pre-war and post-war data.

Tree and forest learners perform brilliantly in-sample, but
are not much more impressive than logistic regression in out-of-sample tests. Due
to the advantage of averaging multiple trees, and reducing overfitting and
variance, forest
performs slightly better than the tree method.

In both tree and forest analysis, I used gini index and entropy and
there is little difference between the AUC.

\subsubsection*{SVM}

When fitting an SVM model with the data, I used two kinds of kernels: rbf
(Gaussian kernel) and sigmoid kernel. The results are both affected by
randomness when fitting the model, but neverthelesss the sigmoid kernel in
general has better out-of-sample perfomance.


\subsection*{A few interesting observations and potential explanations}


When taking a closer look at the results of tree and forest, I found a
few interesting observations of note. Firstly, for both tree and forest, I found the
results are different when fitting models with exactly same data set and
parameters, indicating randomness in the
fitting process. Secondly, for the
models fitted with both lag 2 credit change and credit size, the average
AUC showed fluctuation in the overfitting zone when plotted against maximum depth.
In the forest plot,
all models show fluctuation in the overfitting areas, but for models
with two explanatory variables, the fluctuations have higher amplitudes.
Lastly, for some trees and forests, the out-of-sample AUCs do not drop to 0.5
even when obviously overfitting.

To answer the first question, one first is required to identify the source of
randomness. I found two potential sources for tree and three for
forest by reading the documentation. One obvious candidate is the random start point when searching
for the optimal arguments to minimize the cost function. However, as long as
the problem is convex, the searching results should be within a
relatively small interval with possible differences due to limited resolution. This
doesn't match the observation. As documented as part of \texttt{sklearn.tree.DecisionTreeClassifier}, an argument
called max\_features in tree is defined as ``{[}t{]}he number of features to consider when
looking for the best split''. When fitted with two variables, the model with max
feature set to 1 shows jumps while the model with max features set to 2 does not. For the forest, there is one extra
source of randomness. To grow multiple trees, the sklearn library
bootstraps observations using random selection with replacement. In both tree
and forest functions from the sklearn library, the argument random state controls all the
randomness.

Carrying knowledge mentioned above, I try to decompose the fluctuations.
Fixing random state eliminates fluctuation in
AUC-max depth plot of both tree and forests. For the forest, I first turned off the max
feature limit, and the resulting plot didn't show less fluctuation.
Next, I added the limit on max features and turned off bootstrap, and
this reduced the fluctuation. Eventually, when I turned off both limit
and bootstrap, the fluctuation was almost totally eliminated. These
changes indicates that the
forest fitting is extremely sensitive to changes of training dataset.
Another interesting observation here is that, once both bootstrap and the limit
were turned off, the AUC of models fitted with pre-war data, and with both
credit change and credit size as explanatory variables, dropped below a 0.5
benchmark. I cannot explain this change, relevent plots are in the appendix.

I noticed that models that have
predictive power even when overfitted are models fitted with the whole
dataset. This is true for both tree and forest analysis (with
randomness turn off in forest). From the plot of AUC of trees fitted
with different datasets, I notice that there is only one point in each
of the three lines. This indicates that in the out-of-sample test, only
one split --- presumably the first split --- takes effect, most likely due to
overfitting. The whole set has the largest number of observations, so it is likely that
even when overfitted, the first split still has some predictive power. Here
is one example of an AUC plot when overfitted. This also explains the same
observation in the forest analysis.

\ciapdf{Figure_11.pdf}
\begin{quote}
Figure 7: All three models are fitted with lag 2 credit change and
credit size. These models are fitted with 80\% of the dataset and
tested on 20\% of the dataset. To eliminate the effects of randomness, the
ramdom state argument was fixed when these models were fit.
\end{quote}

\subsection*{Another criteria}

Except for AUC, a few other values from confusion matrix can also be good
criteria for model comparison. In the tables below, I collected some
out-of-sample results for logistic regression and forest fitted with 70\% of
the whole data set regarding confusion matrix. A false alarm is defined as
M01 / (M01 + M11), and can be interpreted as a measure of misclassification
when the model predicts a crisis. Total flag is the percentage of total
observations that have been flagged by the model as a crisis.

To capture 10\% of the crisis,
logistic regression needs to flag 2\% of the total observations, and 80\%
of those flags are false alarms. On the other hand, forest only needs to flag
less than 1\% of the data to achieve this, and only half of those are false alarms. However,
if the goal is to capture half of the crisis, logistic regression only
needs to flag less than 20\% of data with 87\% false alarm while forest
needs to flag half of the data points and more than 90\% of the flags are
false alarms. The main idea here is that there is no single best criteria,
and the criteria selection should be based on the goal of analysis.

\begin{multicols}{2}
\begin{table}[H]
        \caption{Out-of-sample performance for logistic regression based on confusion matrix.
        Logistic regression fitted with 70\% of the whole period and
        tested with 30\%. Explanatory variable is lag 2
        credit change only. AUC is 0.73709.}
        \begin{center}\begin{tabular}{|l|l|l|l|}
        \hline
         threshold         & sensitivity        & falseAlarm         & totalFlag            \\ \hline
        0.95845 & 0.05263 & 0.87500              & 0.01865 \\ \hline
        0.95842 & 0.10526 & 0.80000                & 0.02331 \\ \hline
        0.95829 & 0.15789 & 0.80000                & 0.03497  \\ \hline
        0.95817 & 0.21053 & 0.84000               & 0.05828  \\ \hline
        0.95814 & 0.26316  & 0.83333 & 0.06993  \\ \hline
        0.95805 & 0.31579  & 0.86047 & 0.10023  \\ \hline
        0.95801 & 0.36842  & 0.86000               & 0.11655  \\ \hline
        0.95799 & 0.42105 & 0.85185 & 0.12587   \\ \hline
        0.95796 & 0.47368 & 0.85714 & 0.14685  \\ \hline
        0.95793 & 0.52632  & 0.87179 & 0.18182  \\ \hline
        0.95791 & 0.57895  & 0.88172 & 0.21678  \\ \hline
        0.95790 & 0.63158   & 0.88119 & 0.23543  \\ \hline
        0.95789 & 0.68421  & 0.88288 & 0.25874  \\ \hline
        0.95786 & 0.73684  & 0.89313 & 0.30536  \\ \hline
        0.95773 & 0.78947  & 0.92823 & 0.48718  \\ \hline
        0.95767 & 0.84211  & 0.93701  & 0.59207   \\ \hline
        0.95764   & 0.89474  & 0.93885 & 0.64802   \\ \hline
        0.95759 & 0.94737  & 0.94098  & 0.71096    \\ \hline
        0.95745 & 1.00000                 & 0.95000               & 0.88578   \\ \hline
        \end{tabular}\end{center}
\end{table}

\begin{table}[H]
        \caption{Out-of-sample performance for forest based on confusion matrix. Forest fitted with 70\% of the whole period and tested on
        30\%. Variable is lag 2 credit change only,
        max depth is 3. The AUC of this model is 0.67079.}

    \begin{center}\begin{tabular}{|l|l|l|l|}
    \hline
     threshold           & sensitivity        & falseAlarm         & totalFlag            \\ \hline
    0.31088  & 0.05263 & 0.00000                & 0.00233 \\ \hline
    0.15587  & 0.10526 & 0.50000                & 0.00932 \\ \hline
    0.05381   & 0.15789 & 0.90909 & 0.07692  \\ \hline
    0.04785 & 0.36842  & 0.89394 & 0.15385  \\ \hline
    0.04160  & 0.73684  & 0.93665 & 0.51515   \\ \hline
    0.03808  & 0.84211  & 0.93822 & 0.60373   \\ \hline
    0.03177  & 0.89474  & 0.93885 & 0.64802   \\ \hline
    0.01070 & 0.94737  & 0.95000               & 0.83916   \\ \hline
    0.00829  & 1.00000                 & 0.95571 & 1.00000                  \\ \hline
    \end{tabular}\end{center}
\end{table}
\end{multicols}

\subsection*{Conclusion and potential further questions to answer}

In this study, I implemented
a few supervised learning models and used them on the data. The major predictive
power lies in lag 2 credit change, and this conclusion aligns with the
Schularick and Taylor paper. The predictive power of models is much stronger in-sample. This confirms the result of the Fricke
paper. Tree and forest are particularly
vulnerable to overfitting, and thus require carefully picked parameters like
maximum depth or maximum number of leaves. They are also quite sensitive
to changing training set. Another issue for
tree and forest is that when there is a dominating class in the training
data, the model generated could be biased. This issue can be mitigated by
balancing the data prior to fitting, or by assigning similar weights to different
classes. However, these are not explored in this report and can be
interesting topic for futher study. Since forest shows great potential
to predict crisis, and machine learning methods are not widly use in
economic research, solving this issue could have practical meaning. All
the models tested in this report show some level of predictive power.
However, there is no single standard to judge which is the best model.
The selection criteria strongly depends on the purpose of the analysis
and different models may have advantages in different tasks.

\begin{thebibliography}{9}
\bibitem{booms}
Moritz Schularick \& Alan M. Taylor
\textit{Credit Booms Gone Bust: Monetary Policy, Leverage Cycles, and Financial Crisis, 1870-2008}.

\bibitem{fcp}
Daniel Fricke
\textit{Financial Crisis Prediction: A Model Comparison}.
\end{thebibliography}

\section*{Appendix}

Figure 8: normal depth tree

\ciapdf{app_normaldepth.pdf}

Figure 9: overfitting tree

\vspace*{-\parskip}\resizebox{\textwidth}{!}{\includegraphics{app_overfitting.pdf}}

Figure 10:
different trees generated by same data with limited max feature. We can see
there is a tie in the cost function values.

\ciapdf{app_samedata_1.pdf}

\ciapdf{app_samedata_2.pdf}

\ciapdf{Figure_8.pdf}
\begin{quote}
Figure 11: The maximum feature is limited and bootstrap is on, this is
exactly the same plot used to choose optimal depth. We can see a lot of
fluctuation here and at least three AUCs do not approach the 0.5 benchmark.
\end{quote}

\ciapdf{Figure_9.pdf}
\begin{quote}
Figure 12: The maximum feature is limited and bootstrap is off, and we can
see that fluctuation is reduced.
\end{quote}

\ciapdf{Figure_10.pdf}
\begin{quote}
Figure 13: The maximum feature is unlimited and bootstrap is off, and we
can see dramatic reduction of fluctuation and only two lines do not
approach the 0.5 benchmark.
\end{quote}

\section*{Task 2}

\subsection*{Introduction}

Markovitz portfolio theory has always been a classic. However it is known that
the optimal portfolio is not stable, bringing the issue on frequently rebalancing
for investors. Following the lead of Fastrich, Paterlini, and Winker (Constructing
optimal sparse portfolios using regularization methods, 2014), this study
attempts to find a global minimum variance portfolio under regularizations,
thus stablizing the portfolio choice, and reducing the rebalancing problem.

The regularizations used in this study are Lasso and Ridge. The resulting global
minimum variance portfolios have lower variance than Markovitz portfolios, and
therefore confirm the results from the above paper.

\subsection*{Data description and cleaning}

The data used is French 48 industry portfolios daily data. After comparing the variance
of each industry and across the whole dataset, I chose to use equally weighted data
since it has higher variance, and presumably the difference among regularizations
will be more obvious. After deleting any row that contains missing values, I have
about 44 years' worth of data left, which is a substantial amount.

In figure 1, I plotted the standard deviation of all 48 industies and we can see that ---
except for a few outliers --- most industries standard deviation of returns lies
in a interval between 0.75 to 1.5.

\ciapdf{Figure_1T2.pdf}

\begin{quote}
Figure 1: stddev of 48 industires shows that, the vast majority of industries
have stddev with a steady interval
\end{quote}

Next I plotted mean returns for each of the 48 industries together with the minimum and maximum
returns of each industry. The maximum returns seem to have more fluctuation than
the minimum.

\ciapdf{Figure_2T2.pdf}

\begin{quote}
Figure 2: mean, minimum and maximum returns of each industry
\end{quote}

The industry that has highest variance is coal, whereas utililies has the lowest
variance. Here is a plot to demonstrate that.

\ciapdf{Figure_3T2.pdf}

\begin{quote}
Figure 3: Variances of coal industry and utility industry.
\end{quote}

\subsection*{Global minimum variance portfolio with different regularizations}

\subsubsection*{Three methods to select portfolios}

In this study, I used three methods for selecting minimum variance portfolios.
The first one is Markovitz portfolio selection. The problem here is to minimize variance
of the portfolio under the constraint that sum of all the weights equals to 1.
I also performed selection for long-only portfolios to use together with long-short
portfolio as benchmarks. To find the long-only portfolios, the minimization problem
needs a extra constraint that all the weights must be greater or equal than 0. The second
method I used was Markovitz under Lasso regularization. This is a minimizing
problem, minimizing the sum of variance and a 1-norm penalty:

\begin{equation*}
\mathbf{w^T\sum w} + \lambda \sum_{n= 1}^{K}\left | w_n \right |,
\end{equation*}

where \textbf{w} is weight vector, $\mathbf{\Sigma}$ is the covariance matrix
and K is the number of stocks in the investing universe. Parameter $\lambda$
controls how much penalty to add. To find a portfolio, one needs to
solve this minimizing problem under the same constraint that the sum of weights equals
to one. The other method in this report is a Markovitz under Ridge regularization.
In this minimizing problem, the target function is a sum of variance and 2-norm
penalty:

\begin{equation*}
\mathbf{w^T\sum w} + \lambda \sum_{n= 1}^{K}\left | w_n \right |^2,
\end{equation*}

and solving this under the same budget constraints give a minimum variance
portfolio under Ridge regularization.

\subsubsection*{Selection $\lambda$ with cross validation}

If $\lambda$ equals to 0, both regularized method collapse to classic Markovitz.
On the other hand, if $\lambda$ is too large, which means the penalty term in
the target function will dominate the variance term, it thus imposes too much bias.
To select an optimal $\lambda$, I performed a cross validation as in the Fastrich
paper. This cross validation is only meaningful under stationary assumption.
First, I shuffle the whole training set and divide it into 10 equal folds.
In each round, I take one of these 10 folds as the test set and the other 9 folds as
the training set. Then, in each round, for each value in a series of $\lambda$
that I want to test,
I solve the minimizing problem under relative constraints and thus find a
portfolio, hold this portfolio in the test set, and recieve a series of
returns. I then calculate the
standard deviation of this return series since our aim is to find a minimum
variance portfolio. After 10 rounds, one will have 10 standard deviations for each
$\lambda$, and taking the mean of these gives one the score of that $\lambda$. The
optimal $\lambda$ is the one with the lowest score, and a good searching
interval should give a smile shape curve when plotting all the scores against
the $\lambda$.
If the optimal $\lambda$ is on the edge of the searching interval, a change of interval
is required to find a true minimum score. In the next two figures, I plotted
scores against $\lambda$ for a Lasso penalty and a Ridge penalty.



\ciapdf{Figure_4T2.pdf}

\begin{quote}
Figure 4: portfolio std with Lasso regularizaton. Training period 1 year. Searching
interval $10^{-2}$ to $10^{-1.3}$ and searching steps 100.
\end{quote}


\ciapdf{Figure_5T2.pdf}

\begin{quote}
Figure 5: portfolio std with Ridge regularizaton. Training period 1 year. Searching
interval $10^{-2}$ to $10^{-1.3}$ and searching steps 100.
\end{quote}

Since there is randomness involved in the shuffling step, to find the most reasonable
searching interval, I ran cross validation for both regularizatons multiple times
to make sure that all of the optimal values are within the interval. When using on
the whole data set, if the optimal solution is on the edge, the program will
emit a warning, shift both boundaries of the interval to the coresponding
direction, and will perform another search.

\subsection*{Out-of-sample test}

Out-of-sample tests are performed on the data with various training windows and
holding periods. In the plot below, return series was trained on a one year
window and was held for 3 months.


\ciapdf{Figure_6T2.pdf}

\begin{quote}
Figure 6: return series trained on 250 days and hold for 63 days. Cross validation
interval, Lasso: $10^{-2.1}$ to $10^{-1.2}$, Ridge: $10^{-2.3}$ to $10^{-1.3}$.
Searching steps: 100 for both.
\end{quote}

The variance of portfolios are 4.5392, 3.9004, 3.6182 and 3.3498 in percentage
respectively
for Markovitz long-short, Markovitz long-only, Lasso, and Ridge. As expected,
portfolios found under regularizations have lower variance during the holding
period. This result comes about as the barrier set by the penalty term blocks
the noise in the input data (which is thus also in the variance covariance
matrix) from entering the results freely. A small surprise here is that with
more constraints, the long-only portfoio has lower variance than the long-short
portfolio. This is not consistent across all the of backtests I performed, but it
indicates the instability of Markovitz's solution.

Next, I performed a series of out-of-sample tests with moving windows. I tested
different training windows to see whether having a longer training window affects
any methods. I also tested different holding periods to see whether some methods
perform better when the selected portfolios were rebalanced more often. Since
the Fastrich paper only used 5 to 6 years worth of data, and considering the
running time, I
decided to run these tests on the last 5 years from the whole data set.


\subsubsection*{\centering{}Out-of-sample test result}

\begin{table}[H]
    \caption{Out-of-sample test with moving windows. The last 1250
    observations of the data set are used}
    \begin{center}\begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    Training window (days) & Holding window (days) & window number & Marlovitz long-short & Markovitz long-only & Lasso & Ridge \\ \hline
    500 & 63 & 11  & 3.5831 & 4.3738 & 3.4346 & 3.4377 \\ \hline
    250 & 63 & 15  & 3.4866 & 4.3458 & 3.2998 & 3.2627 \\ \hline
    125 & 63 & 17  & 4.1919 & 4.3288 & 3.4325 & 3.4712 \\ \hline
    250 & 125 & 8  & 3.9520 & 4.5060 & 3.6145 & 3.7762 \\ \hline
    250 & 63 & 15  & 3.4866 & 4.3459 & 3.2840 & 3.2682 \\ \hline
    250 & 21 & 47  & 3.3769 & 4.1968 & 3.1406 & 3.1526 \\ \hline
    \end{tabular}\end{center}
\end{table}

In this table, we can see that the result is robust to the change of
training table and holding period. In all of the tests, across various training
periods
and holding lengths, the moving window out-of-sample tests demonstrate that
regularized portfolios constently outperform the Markovitz portfolio. Among
two benchmarks, long-only portfolio has higher variance because it cannot
take advantage of shorting, and thus has less ability to diversify the risk. Between
Lasso and Ridge, there is no clear result indicating which one provides portfolios
with lower variance. The first three rows of the table seem to indicate that
shorter training periods make the regularized portfolios outperform their
competitors even more.
This is not enough to evidence to form a conclusion, but one potential reason could be
that in a short training period, the valid information in the covariance matrix
is more muddled with noise and thus the Markovitz method has more difficulty
to select real optimal portfolio. In addition, because of the penalty term, regularized
methods have more restrictive standards which permit only significant information to
pass through, thus allowing them to outperform Markovitz portfolios even more.
When changing holding periods, there is no clear result about whether a certain
length of holding benifits a particular method.

Finally, I performed a out-of-sample test on the entire 44 years of data. The
resulting variances were 2.1290, 3.5056, 2.0711, and 2.1074 for Markovitz long-short,
Markovitz long-only, Lasso, and Ridge respectively. As usual, portfolios under
regularizations
outperformed both benchmarks. The order of variances for the four portfolios
and magnitude of differences
match the results of the Fastrich, Paterlini and Winker paper.

Both the Fastrich paper and Brodie paper mentioned sparseness as another
feature of portfolios chosen by regularized Markovitz methods, however, with
the threshold set at 0,
neither set of portfolios selected under regularizations demonstrate this feature. I
did not explore further from here, but it might be related to the solver used. Also,
a position with weight less than $10^{-2}$ is practically non-active, so the
sparse feature can also rely on the choice of threshold.

\subsection*{Conclusion and potential further questions to answer}

While observing the global minimum variance portfolios found under different regularizations,
I confirmed that regularized portfolios outperform both Markovitz benchmarks
in the out-of-sample tests, and as such found optimal portfolios have lower variance in various
the holding periods. Markovitz solutions are not stable and contains noise from the
data so regularization provides practical benefit to investors. Another benefit
of regularizations proposed by both the Fastrich paper and Brodie paper is that
they give
sparse portfolios, thus reducing the transition cost. This is not observed here
and could, potentially, be a interesting further topic.

\begin{thebibliography}{9}
\bibitem{booms}
B.Fastrich, S.Paterlini \& P.Winker (2014)
\textit{Constructing optimal sparse portfolios using regularization methods}.

\bibitem{fcp}
J.Brodie et al. (2009)
\textit{Sparse and stable Markovitz portfolios}.
\end{thebibliography}

\end{document}
